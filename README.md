# LLM Benchmark

This project benchmarks small LLMs running locally with Ollama. It tests response time, model load time, and performance under various conditions.
